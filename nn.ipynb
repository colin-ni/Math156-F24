{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import abc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The language of modules\n",
    "Most model architectures in modern deep learning can be thought of as a\n",
    "composition of _modules_.\n",
    "\n",
    "**Definition.** A _module_ refers to a function $X \\times W \\to Y,$\n",
    "where $X$ denotes the space of inputs and $W$ denotes the space of learnable\n",
    "weights.\n",
    "\n",
    "**Example.** The activation function ReLU is a module\n",
    "$X = \\mathbb{R} \\to \\mathbb{R}$ with no learnable weights, as is Softmax $X = \\mathbb{R}^c \\to \\mathbb{R},$\n",
    "where $c$ denotes the number of classes.\n",
    "\n",
    "**Example.** The loss function MSE is a\n",
    "module $X = \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ with no learnable weights, as is\n",
    "CrossEntropy $X = \\mathbb{R}^c \\times \\{{\\bf e}_1, \\dotsc, {\\bf e}_c\\} \\to \\mathbb{R}.$\n",
    "\n",
    "**Example.** A linear module $\\text{Linear}(a, b)$ with $a$ in-features and\n",
    "$b$ out-features is a module\n",
    "$X \\times W = \\mathbb{R}^a \\times (\\mathbb{R}^{b \\times a} \\times \\mathbb{R}^b) \\to \\mathbb{R}^b$\n",
    "defined as $({\\bf x}, ({\\bf W}, {\\bf b})) \\mapsto {\\bf W}{\\bf x} + {\\bf b}.$\n",
    "It has precisely $(a + 1)b$ learnable weights.\n",
    "\n",
    "Composing modules produces new modules, accummulating the learnable weights and\n",
    "passing unused ones into the next layers. More precisely, given modules\n",
    "$X \\times V \\xrightarrow{f} Y$ and $Y \\times W \\xrightarrow{g} Z,$ their\n",
    "composition $g \\circ f$ is defined by\n",
    "$$X \\times (V \\times W) \\xrightarrow{f \\times 1} Y \\times W \\xrightarrow{g} Z \\xrightarrow{L} \\mathbb{R}.$$\n",
    "\n",
    "**Key Example.** A neural network is a certain sequence of these modules, such\n",
    "as the following one with 1 hidden layer of size $h$ that can perform\n",
    "classification on data in $\\mathbb{R}^n$ for $c$ classes:\n",
    "$$\\text{Softmax} \\circ \\text{Linear}(h, c) \\circ \\text{ReLU} \\circ \\text{Linear}(n, h).$$\n",
    "There are exactly $(n + 1)h + (h + 1)c$ learnable weights in this module,\n",
    "coming from the two linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a module\n",
    "\n",
    "**Strategy.** Let $X \\times U \\to Y$ be a module, which we will think of as complicated,\n",
    "e.g. a deep neural network. We will learn the weights by choosing a loss\n",
    "function $Y \\xrightarrow{L} \\mathbb{R}$ and minimizing\n",
    "$X \\times U \\to Y \\xrightarrow{L} \\mathbb{R}$\n",
    "by performing gradient descent on the weights $U.$ To compute the gradient with\n",
    "respect to $u,$ we divide and conquer by expressing the complicated module as a\n",
    "composition $g \\circ f$ of simpler modules $X \\times V \\xrightarrow{f} Y$ and\n",
    "$Y \\times W \\xrightarrow{g} Z$ where $U = V \\times W$ and using chain rule:\n",
    "$$(\\nabla_w Lgf)(x, v, w) = (\\nabla_w Lg)(f(x, v), w),$$\n",
    "$$(\\nabla_v Lgf)(x, v, w) = (\\nabla_y Lg)(f(x, v), w) \\cdot (\\nabla_v f)(x, v).$$\n",
    "The latter gradient is the nontrivial one to compute, and we call this\n",
    "`grad_weights`.\n",
    "\n",
    "**Details.**\n",
    "In the above expression for `grad_weights` we have two things that cannot be\n",
    "computed locally in the recursion:\n",
    "- the term $f(x, v)$ which we call `output`,\n",
    "- the crucial $(\\nabla_y Lg)(f(x, v), w)$ term which we call `grad_output`.\n",
    "\n",
    "To obtain `output`, we simply feed $(x, v, w)$ through the module before\n",
    "starting the recursion and store the outputs at each submodule. To obtain\n",
    "`grad_output`, we bravely assume it comes with the recursion, so we must compute\n",
    "and pass on the gradient\n",
    "$$(\\nabla_x Lgf)(x, v, w) = (\\nabla_y Lg)(f(x, v), w) \\cdot (\\nabla_x f)(x, v)$$\n",
    "of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Here is some elementary code to generate toy data and train and test a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, n_classes):\n",
    "    return np.identity(n_classes)[y.astype(int)]\n",
    "\n",
    "def generate_data(dataset_name, n_samples=100):\n",
    "    match dataset_name:\n",
    "        case 'regression: an affine line':\n",
    "            X = np.random.uniform(-5, 5, size=(n_samples, 1))\n",
    "            y = 3 * X - 100\n",
    "        case 'regression: cubic polynomial':\n",
    "            X = np.random.uniform(-5, 5, size=(n_samples, 1))\n",
    "            y = X ** 3 - 2 * X + 5\n",
    "        case 'classification: linearly separable':\n",
    "            X = np.random.normal(size=(n_samples, 2))\n",
    "            y = one_hot_encode((X[:, 0] > X[:, 1] + 1).astype(int), 2)\n",
    "        case 'classification: XOR':\n",
    "            X = np.random.normal(size=(n_samples, 2))\n",
    "            y = one_hot_encode((X[:, 0] * X[:, 1] > 0).astype(int), 2)\n",
    "        case _:\n",
    "            raise ValueError('invalid dataset name')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_and_test(model, loss_fn, optimizer, dataset_name, n_epochs=5000, n_samples=100):\n",
    "    # train\n",
    "    X_train, y_train = generate_data(dataset_name, n_samples=n_samples)\n",
    "    losses = []\n",
    "    for _ in tqdm(range(n_epochs)):\n",
    "        output = model.forward(X_train)\n",
    "        loss = loss_fn.forward(output, y_train) \n",
    "        losses.append(loss)\n",
    "        model.backward(loss_fn.backward())\n",
    "        optimizer.step()\n",
    "\n",
    "    # test and plot\n",
    "    X_test, y_test = generate_data(dataset_name, n_samples=n_samples)\n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "    fig.set_size_inches(10, 5)\n",
    "    ax[0].set_title('loss during training')\n",
    "    ax[1].set_title('predictions and truth')\n",
    "    if dataset_name.startswith('classification'):\n",
    "        y_pred = np.argmax(model.forward(X_test), axis=1)\n",
    "        sns.lineplot(losses, ax=ax[0])\n",
    "        sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1], hue=np.argmax(y_test, axis=1), ax=ax[1])\n",
    "        sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1], hue=y_pred, s=100, alpha=0.2, ax=ax[1])\n",
    "        plt.legend([],[], frameon=False)\n",
    "    elif dataset_name.startswith('regression'):\n",
    "        y_pred = model.forward(X_test)\n",
    "        sns.lineplot(losses, ax=ax[0])\n",
    "        sns.scatterplot(x=X_test[:, 0], y=y_test[:, 0], ax=ax[1])\n",
    "        sns.scatterplot(x=X_test[:, 0], y=y_pred[:, 0], s=100, alpha=0.2, ax=ax[1])\n",
    "        plt.legend([],[], frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the general `Module`, `Composition`, and `SGD` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    # require inheriting classes to implement the abstract methods\n",
    "    __metaclass__ = abc.ABCMeta \n",
    "\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        Stores learnable weights in a dictionary `self.weights` keyed by\n",
    "        strings.\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.weights = {}\n",
    "        self.grad_weights = {}\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Computes the output of the module, then stores the output in\n",
    "        `self.output` and returns it.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def backward(self, grad_output):\n",
    "        '''\n",
    "        Computes `self.grad_weights` organized as a dictionary in the same\n",
    "        way as `self.weights` and returns `grad_input`.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class Composition(Module): # in PyTorch this is called `Sequential`\n",
    "    pass # TODO\n",
    "    \n",
    "\n",
    "class SGD:\n",
    "\n",
    "    def __init__(self, module, lr=1e-3):\n",
    "        self.module = module\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        pass # TODO\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is MSE which has the straightforward gradient\n",
    "$$(\\nabla_{\\bf y} \\text{MSE})({\\bf y}, {\\bf t}) = {2 \\over b} ({\\bf y} - {\\bf t}).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Module):\n",
    "\n",
    "    pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a linear layer, the gradients with respect to the weights are\n",
    "$$(\\nabla_{\\bf W} \\text{Linear})({\\bf x}, ({\\bf W}, {\\bf b})) = {\\bf x} \\hspace{0.5cm}\\text{and}\\hspace{0.5cm} (\\nabla_{\\bf b} \\text{Linear})({\\bf x}, ({\\bf W}, {\\bf b})) = {\\bf 1},$$\n",
    "and the gradient with respect to the input is\n",
    "$$(\\nabla_{\\bf x} \\text{Linear})({\\bf x}, ({\\bf W}, {\\bf b})) = {\\bf W}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, n_in_features, n_out_features):\n",
    "        super().__init__(f'Linear({n_in_features}, {n_out_features})')\n",
    "        self.n_in_features = n_in_features\n",
    "        self.n_out_features = n_out_features\n",
    "        \n",
    "        # Initialize weights so that they are uniformly in the unit ball. Don't\n",
    "        # worry about this choice; for examples of other choices, see\n",
    "        # https://pytorch.org/docs/stable/nn.init.html\n",
    "        pass # TODO\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass # TODO\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test.** Check that you can learn an affine line. This requires `Composition`,\n",
    "`Linear`, `MSE`, and `SGD` to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Composition([\n",
    "    Linear(1, 8),\n",
    "    Linear(8, 1)\n",
    "])\n",
    "\n",
    "train_and_test(\n",
    "    model=model,\n",
    "    loss_fn=MSE(),\n",
    "    optimizer=SGD(model, lr=1e-3),\n",
    "    dataset_name='regression: an affine line',\n",
    "    n_epochs=100,\n",
    "    n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the usual activation functions implemented as `Module`s. The gradient\n",
    "for `ReLU` is easy, and the gradient for `Softmax` is\n",
    "$$(\\nabla \\text{Softmax})({\\bf a}) = \\text{diag}({\\bf a}) - {\\bf a}{\\bf a}^T.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__(f'Softmax({n_classes})')\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass # TODO\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        # hacks :)\n",
    "        s = self.output\n",
    "        diag = np.einsum('ij, jk -> ijk', s, np.identity(s.shape[1]))\n",
    "        outer = np.einsum('ij, ik -> ijk', s, s)\n",
    "        grad_softmax = diag - outer\n",
    "        return np.einsum('ijk, ik -> ij', grad_softmax, grad_output)\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__('ReLU')\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass # TODO\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test.** Check that you can learn a cubic polynomial. This requires `ReLU`\n",
    "to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Composition([\n",
    "    Linear(1, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 8),\n",
    "    ReLU(),\n",
    "    Linear(8, 1),\n",
    "])\n",
    "\n",
    "train_and_test(\n",
    "    model,\n",
    "    MSE(),\n",
    "    SGD(model, lr=1e-4),\n",
    "    'regression: cubic polynomial',\n",
    "    n_epochs=5000,\n",
    "    n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented the finnicky Softmax module and recalled the gradient of MSE,\n",
    "we've arrived at the following extremely important point:\n",
    "\n",
    "**Important clarification.** Recall the nice identity showing that the gradient\n",
    "of CrossEntropy is, up to a scalar, the gradient of MSE:\n",
    "$$\\nabla_{\\bf y} (\\text{CrossEntropy} \\circ \\text{Softmax})({\\bf y}, {\\bf t}) = {1 \\over b}(\\text{Softmax}({\\bf y}) - {\\bf t}).$$\n",
    "Many sources, including the textbook and the previous sentence, are vague (or\n",
    "often just wrong) when they say this. The things to remember are the following:\n",
    "- the gradient is of the composition of Softmax followed by CrossEntropy\n",
    "- the gradient is being taken with respect to the input to the Softmax\n",
    "- the term in the RHS involving the input is the Softmax of the input.\n",
    "\n",
    "**Remark.** There is even confusion about this in practice. Since Softmax is almost\n",
    "always directly followed by CrossEntropy, these are typically implemented as\n",
    "one module; in fact, one of my pet peeves about PyTorch is that their\n",
    "`nn.CrossEntropyLoss` includes the Softmax! Moreover, since accidentally taking\n",
    "Softmax twice (e.g. applying Softmax before calling PyTorch's\n",
    "`nn.CrossEntropyLoss`) causes moderate but not horrifyingly-bad problems,\n",
    "often such mistakes go unresolved.\n",
    "\n",
    "Inspired by this nice identity that we have (only) for the gradient of the\n",
    "composition, we will implement Softmax and CrossEntropy as one module; this \n",
    "indeed throws away the hard work we did for the backward method of Softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Module):\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__(f'CrossEntropy({n_classes})')\n",
    "        pass # TODO\n",
    "\n",
    "    def forward(self, input, t):\n",
    "        pass # TODO\n",
    "    \n",
    "    def backward(self):\n",
    "        pass # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test.** Check that you can classify linearly separable data. This requires\n",
    "`SoftmaxCrossEntropy` to work but no activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Composition([\n",
    "    Linear(2, 8),\n",
    "    Linear(8, 2),\n",
    "])\n",
    "\n",
    "train_and_test(\n",
    "    model,\n",
    "    SoftmaxCrossEntropy(2),\n",
    "    SGD(model, lr=1e-2),\n",
    "    'classification: linearly separable',\n",
    "    n_epochs=1000,\n",
    "    n_samples=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test.** Check that you can learn XOR. This requires everything to work, and you'll need to adjust the hyperparameters `H` and `n_epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 32\n",
    "\n",
    "model = Composition([\n",
    "    Linear(2, H),\n",
    "    ReLU(),\n",
    "    Linear(H, H),\n",
    "    ReLU(),\n",
    "    Linear(H, 2)\n",
    "])\n",
    "\n",
    "train_and_test(\n",
    "    model,\n",
    "    SoftmaxCrossEntropy(2),\n",
    "    SGD(model, lr=1e-2),\n",
    "    'classification: XOR',\n",
    "    n_epochs=5000,\n",
    "    n_samples=250\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
